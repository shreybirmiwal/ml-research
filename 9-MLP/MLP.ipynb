{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d44168-3480-4018-9d42-e2fd5b2a8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "words = []\n",
    "with open(\"google-10000-english-no-swears.txt\", 'r') as file:\n",
    "    words = file.read().splitlines()\n",
    "    \n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c577368a-8965-4ee8-819b-0a24fbdb9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSplitData(words):\n",
    "    \n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = \"{{{\"+word+\"{\"\n",
    "\n",
    "        for i in range(len(word)-3):\n",
    "            #print(word[i], word[i+1], word[i+2], \"-->\", word[i+3])\n",
    "            \n",
    "            x = [ord(word[i])-97, ord(word[i+1])-97, ord(word[i+2])-97]\n",
    "            y = [ord(word[i+3])-97]\n",
    "\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    # print(xs)\n",
    "    # print(ys)\n",
    "    return (xs, ys)\n",
    "            \n",
    "data = getSplitData(words[:10000])\n",
    "xs = data[0]\n",
    "ys = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85295fcc-4f4e-493d-9d34-fb14fe73a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75153, 3])\n",
      "torch.Size([75153, 1])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(xs.size())\n",
    "print(ys.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428d5a7-e599-4d64-a0c2-61a391c8db53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs torch.Size([75153, 27])\n",
      "Train # 0  Loss:  3.0778920650482178\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 1  Loss:  2.947154998779297\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 2  Loss:  2.833498954772949\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 3  Loss:  2.736640691757202\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 4  Loss:  2.6558570861816406\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 5  Loss:  2.584667921066284\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 6  Loss:  2.5112318992614746\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 7  Loss:  2.4323761463165283\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 8  Loss:  2.367539882659912\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 9  Loss:  2.3268682956695557\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 10  Loss:  2.2998104095458984\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 11  Loss:  2.2795698642730713\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 12  Loss:  2.263108253479004\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 13  Loss:  2.248861312866211\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 14  Loss:  2.2360029220581055\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 15  Loss:  2.22434139251709\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 16  Loss:  2.213900327682495\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 17  Loss:  2.2046356201171875\n",
      "probs torch.Size([75153, 27])\n",
      "Train # 18  Loss:  2.1964199542999268\n"
     ]
    }
   ],
   "source": [
    "# 1. first we vector embbed\n",
    "# 2. then we do first layer and bias\n",
    "# 3. then we tanh\n",
    "# 4. then we do 2nd layer and bias and shrink down to just 27 outputs\n",
    "# 5. then we softmax \n",
    "# 6. then we get loss \n",
    "\n",
    "#tunable params: embedding_table, w1, b1, w2, b2\n",
    "\n",
    "embedding_table = torch.randn((27, 2), requires_grad=True)\n",
    "\n",
    "W1 = torch.randn((6, 100), requires_grad=True)\n",
    "B1 = torch.randn((100), requires_grad=True)\n",
    "\n",
    "W2 = torch.randn((100, 27), requires_grad=True)\n",
    "B2 = torch.randn((27), requires_grad=True)\n",
    "\n",
    "params = (W1, B1, W2, B2, embedding_table)\n",
    "\n",
    "\n",
    "for _ in range(5000):\n",
    "\n",
    "    embedded_data = embedding_table[xs] #look up  for each xs find embedding\n",
    "    embedded_data = embedded_data.view(embedded_data.shape[0], 6)\n",
    "\n",
    "    \n",
    "    layer1 = embedded_data @ W1 + B1\n",
    "    tan_layer1 = torch.tanh(layer1)\n",
    "    #print(\"Tan layer 1\", tan_layer1.size())\n",
    "    \n",
    "    \n",
    "    layer2 = tan_layer1 @ W2 + B2\n",
    "    #print(\"Layer 2\", layer2.size())\n",
    "    \n",
    "    #softmax to do the loss: But remember, this is the same as cross-entropy-loss: which is actually more efficient cuz pytorch implemntaiton\n",
    "    \n",
    "    #layer2 is basically the logits \n",
    "    counts = layer2.exp()\n",
    "    #we need to make counts the probs\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    print(\"probs\", probs.size())\n",
    "    \n",
    "    \n",
    "    #loss function here: we basically want the probs predicted for the actual output to be max. \n",
    "    # to start: get the mean of the probs predicted for the actual output\n",
    "    sum = 0\n",
    "    for i in range(probs.shape[0]):\n",
    "        actual_output = ys[i]\n",
    "        predicted_prob_of_actual_output = probs[i][actual_output]\n",
    "        sum += predicted_prob_of_actual_output\n",
    "    \n",
    "    sum = sum / probs.shape[0]\n",
    "    \n",
    "    #now we have sum, higher sum is good. but we need to turn into loss where lower loss is better. so we -negativeloglikelyhood\n",
    "    neg_log_likelihood = -sum.log()\n",
    "\n",
    "    \n",
    "    print(\"Train #\", _ , \" Loss: \", neg_log_likelihood.item())\n",
    "\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "        \n",
    "    neg_log_likelihood.backward()\n",
    "    \n",
    "    for param in params:\n",
    "        param.data += -.1 * param.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
